{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "#### The hypothesis function\n",
    "\n",
    "$$h_\\theta (x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "$$J(\\theta_{0},\\theta_{1})=\\frac{1}{2m}\\sum_{i=1}^{m}\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^{2}$$\n",
    "\n",
    "#### Gradient descent\n",
    "Update $\\theta_0$ and $\\theta_1$ simultaniousely, until convergence:\n",
    "$$\\theta_{j}:=\\theta_{j}-\\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta_{0},\\theta_{1})$$\n",
    "\n",
    "Multiple variables and solve the differentiation.\n",
    "Repeat until convergence:{\n",
    "$$\\theta_{j}:=\\theta_{j}-\\frac{\\alpha}{m}\\sum_{i=1}^{m}\\left(\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)x_{j}^{(i)}\\right)$$\n",
    "}\n",
    "\n",
    "Here, i = 1:m is the number of observations, j is the number of variables (with 1 as the first one), $\\alpha$ is the learning step.\n",
    "\n",
    "Variable (feature) **scaling** can help gradient descent converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# matlab code\n",
    "m = length(y); % number of training examples\n",
    "num_iters = 1000;\n",
    "\n",
    "% gradient descent to find thetas that will minimize cost\n",
    "J_history = zeros(num_iters, 1); % to host cost of each comb of theta\n",
    "for iter = 1:num_iters\n",
    "    diffe = X * theta - y;\n",
    "    % update both theta at the same time.\n",
    "    theta(1) = theta(1) - (alpha/m) * sum(diffe .* X(:, 1));\n",
    "    theta(2) = theta(2) - (alpha/m) * sum(diffe .* X(:, 2));\n",
    "    J_history(iter) = sum((X * theta - y).^2)/(2*m);\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Normal equation**: $\\theta = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "For large sample size, the normal equation will be very slow to calculate ($(X^TX)^{-1}$). But the gradient descent will work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "The outcomes are either 1 or 0. So, we need $0\\leq h(\\theta) \\leq 1$. We use logistic regression.\n",
    "$$h_{\\theta}(x)=g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots) = g(\\theta^{T}x)=P(y=1|x;\\theta)$$\n",
    "$$g(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function\n",
    "If we still use the same cost function as linear regression, the the $J(\\theta)$ vs $\\theta$ is non-convex, it will have many local minimum.\n",
    "\n",
    "Logistic regression cost function:\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\textrm{Cost}(h_{\\theta}(x^{(i)}),y^{(i)})$$\n",
    "\n",
    "$$\\textrm{Cost}(h_{\\theta}(x),y)=\\begin{cases}\n",
    "-\\log(h_{\\theta}(x)) & if\\;y=1\\\\\n",
    "-\\log(1-h_{\\theta}(x)) & if\\;y=0\n",
    "\\end{cases}$$ \n",
    "\n",
    "In this way, if y = 1 will predict = 0, then the cost is approaching infinity.\n",
    "\n",
    "Put both conditions together (y can only be either 1 or 0):\n",
    "\n",
    "$$\\textrm{Cost}(h_{\\theta}(x),y)=-y\\log(h_{\\theta}(x))-(1-y)\\log(1-h_{\\theta}(x))$$\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m}\\left[\\sum_{i=1}^{m}y^{(i)}\\log(h_{\\theta}(x^{(i)}))-(1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))\\right]$$\n",
    "\n",
    "#### Gradient descent\n",
    "Our **goal** is to get the $\\theta$ that minimize $J(\\theta)$: $\\min _\\theta J(\\theta)$.\n",
    "\n",
    "We need code that can compute $J(\\theta)$ and $\\partial J(\\theta)/\\partial \\theta_j$\n",
    "\n",
    "Repeat many times and simultaneously update all $\\theta_j$.\n",
    "$$\\theta_{j}:=\\theta_{j}-\\frac{\\alpha}{m}\\sum_{i=1}^{m}\\left(\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)x_{j}^{(i)}\\right)$$\n",
    "\n",
    "It looks identical to linear regression, but here $h_\\theta (x) = \\frac{1}{1+e^{-z}}$ instead of $\\theta^Tx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization algorithm\n",
    "\n",
    "- Gradient descent\n",
    "- Conjugate gradient\n",
    "- BFGS\n",
    "- L-BFGS\n",
    "\n",
    "The 2-4 do not need to manually pick $\\alpha$ and often faster than gradient descent, but they are more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "E.g. weather: sunny (1), cloudy (2), rain (3), snow (4), etc.\n",
    "\n",
    "##### One-vs-all (one-vs-rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
